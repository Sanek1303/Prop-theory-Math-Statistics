{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка статистических гипотез. Проверка гипотезы о виде распределения\n",
    "\n",
    "1. Общие сведения\n",
    "2. Критерий Колмогорова\n",
    "3. Критерий $\\chi^2$ (Пирсона)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $X$ - данные, $\\{x\\}$ - выборочное пространство, $\\mathcal{F} = \\{F\\}$ - совокупность априори допустимых распределений $X$. $F_X$ - неизвестное истинное рпспределение данных $X, F_X \\in \\{F\\}$. В общем случае задача проверики гипотез ставится так: выделяется некоторое подмножество $\\mathcal{F_0 \\subset F}$ допустимых распределений и требуется по данным $X$ проверить, справедливо ли утверждение $H_0: F_X \\in \\mathcal{F_0}$ или же оно ложно. В этом случае $H_0$ называется оносновной (нулевой) гипотезой. Распределения $F \\in \\mathcal{F_1 = F \\backslash F_0}$ называеются альтернативными, а утверждение $H_1: F_X \\in \\mathcal{F_1}$ - альтернативной гипотезой. В этом случае речь идет о проверке гипотезы $H_0$ против альтернативы $H_1$ или о задаче $(H_0, H_1)$; иногда так же говорят, что гипотеза $H_0$ проверяется внутри обзей гипотезы $H: F_X \\in \\mathcal{F = F_1 \\cup F_0}$. Если подмножество $\\mathcal{F_0(F_1)}$ состоит из одного элемента, то гипотеза $H_0$ (альтернатива $H_1$) называется простой, в противном случае сложной.   \n",
    "\n",
    "Правило, согласно которому мы, наблюдая $X$, принимаем решение принять гипотезу $H_0$ как истинную либо отклонить ее как ложную (т.е. принять альтернативную гипотезу $H_1$) называется **статистическим критерием** (или просто критерием).\n",
    "\n",
    "В некоторых случаях класс $\\mathcal{F}$ - это все распределения на выборочном пространстве $\\{x\\}$, тогда альтернативная гипотеза $H_1 = \\bar H_0$ никак не конкретизируется, и здесь речь идет просто о согласии данных $X$ с нулевой гипотезой $H_0$ (согласуются ли данные $X$ с гипотезой $H_0$ или же они ее опровергают) - соответствующие критерии называются критериями согласия.  \n",
    "Поскольку критерий - это правило, которое для каждoй реализации $x$ выборки $X$ должно приводить к одному из двух решений: принять гипотезу $H_0$ или отклонить ее (принять альтернативу $H_1$), то каждому критерию соответствует некоторое разбиение выборочного пространства $\\mathcal{X} = \\{x\\}$ на два взаимно дополнительных множества $\\mathcal{X_0 \\cap X_1 = \\emptyset, X_0 \\cup X_1 = X}$. Где $\\mathcal{X_0}$ состоит из тех точек $x$, для которых $H_0$ принимается, а $\\mathcal{X_1}$ - из тех, для которых $H_0$ отвергается (принимается $H_1$). Таким образом, $\\mathcal{X_0}$ - это область принятия гипотезы $H_0$, а $\\mathcal{X_1}$ - область ее отклонения, которую принято называть **критической областью**.  \n",
    "Тем самым, любой критерий проверки гипотезы $H_0$ однозначно задается соответствующей критической областью $\\mathcal{X_1}$ и о таком критерии часто говорят как о \"критерии $\\mathcal{X_1}$\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, критерий $\\mathcal{X_1}$ имеет вид:  \n",
    "  \n",
    "$$ H_0  \\; отвергается  \\Leftrightarrow X \\in \\mathcal{X_1}$$  \n",
    "\n",
    "Если в эксперименте наблюдается маловероятное при справедливости гипотезы $H_0$ событие, то считается, что гипотеза $H_0$ не согласуется с данными (или противоречит им), и в данном случае она отвергается (отлоняется); в противном случае считается, что данные не противоречат $H_0$ (или согласуется с ней), и $H_0$ принимается.\n",
    "  В соответсвии с этим приницпом критическая область $\\mathcal{X_1}$ должна быть выбрана так, чтобы была мала вероятность $P \\{X \\in \\mathcal{X_1} | H_0\\}$, т.е. условная (при условии, что $H_0$ спрведлива) вероятность попадания значения выборки $X$ в область $\\mathcal{X_1}$. Поэтому при построении критерия задаются заранее некоторым малым числов $\\alpha$ (например: $\\alpha = 0.001; 0.01; 0.05  \\; и \\; т.д.$) и налагают условие:  \n",
    "(1)  \n",
    "  $$ P\\{X \\in \\mathcal{X_1}| H_0\\} \\leq \\alpha$$  \n",
    "Если выполнено (1), то говорят, что критерий $\\mathcal{X_1}$ имеет уровень значимости $\\alpha$ и подчеркивают это обозначением $\\mathcal{X_1 = X_{1\\alpha}} $. Ясно, что условием (1) критическая область $\\mathcal{X_{1\\alpha}}$ определяется неоднозначно, и чтобы утсранить эту неопределенность, надо ввести дополнительное понятие ошибок критерия.   \n",
    "**Ошибка 1-го рода** - отвергуть $H_0$ при условии, что она верна. Например, в биомтерических технологиях она называется FRR (false rejection rate) и  означает, что человек не распознан системой.  \n",
    "**Ошибка 2-го рода** - принять $H_0$ при условии, что она неверна.  \n",
    "Например, в биометрических технологиях она называется FAR (false acceptance rate) и означает, что один человек принят системой за другого. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введем теперь фундаментальное понятие теории проверки статистических гипотез - понятие фукнции мощности критерия. По определению, функцией мощности критерия $\\mathcal{X_{1\\alpha}}$ называется следующий функционал на множестве всех допустимых распределний $\\mathcal{F} = \\{F\\}$ выборки $X$:  \n",
    "(2)  \n",
    "$$W(F) = W(F; \\mathcal{X_{1\\alpha}}) = P\\{X \\in \\mathcal{X_{1\\alpha}} | F \\}, F \\in \\mathcal{F}$$   \n",
    "\n",
    "ДРугими словами, $W(F; \\mathcal{X_{1\\alpha}})$ - это вероятность попадания значения выборки $X$ в критическую область $\\mathcal{X_{1\\alpha}}$, когда $F$ - ее истинное распределение. Через функцию мощности легко выразить вероятности обоих типов ошибок, свойственных критерию $\\mathcal{X_{1\\alpha}}$. Именно, вероятность ошибки 1-го рода есть $W(F)$ при $F \\in \\mathcal{F_0}$, а второго рода $1-W(F)$ при $F \\in \\mathcal{F_1}$.  \n",
    "  \n",
    "$$ P\\{H_1 | H_0 \\} \\; - вероятность \\; ошибки \\; 1-го \\; рода  $$\n",
    "  \n",
    "$$ P\\{H_0 | H_1 \\} \\; -  вероятность \\; ошибки \\; 2-го \\; рода $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Несмещенность критерия** - одновременное выполнение условий:  \n",
    "  \n",
    "$$W(F; \\mathcal{X_{1\\alpha}}) \\leq \\alpha, \\forall F \\in \\mathcal{F_0}, W(F; \\mathcal{X_{1\\alpha}}) > \\alpha, \\forall F \\in \\mathcal{F_1}$$ \n",
    "\n",
    "**Состоятельность критерия** - \n",
    "$$ W_n(F; \\mathcal{X_{1\\alpha}}) \\rightarrow 1, \\forall F \\in \\mathcal{F_1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерий Колмогорова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот критерий примеяют в тех случаях, когда функция $F(x)$ непрерывна. Статистика критерия определяется формулой:  \n",
    "(1)  \n",
    "$$ D_n = D_n(X) = \\sup_{-\\infty<x<+\\infty} |\\hat F_n(x) - F(x)|$$  \n",
    "Где $\\hat F(x)$ - эмпирическая функция распределения  \n",
    "  \n",
    "И по теореме Колмогорова:  \n",
    "$$ \\forall t > 0 : \\lim_{n \\rightarrow \\infty} \\; P(\\sqrt{n}D_n \\leq t) = K(t) = \\sum_{j = -\\infty}^{+\\infty} (-1)^{j} e^{-2j^2t^2}$$  \n",
    "\n",
    "$$\\mathcal{T_{1\\alpha}} = \\{ t \\geq t_{\\alpha}, t_{\\alpha} = \\lambda_{\\alpha}/\\sqrt{n} \\}$$  \n",
    "  \n",
    "$$P\\{D_n \\in \\mathcal{T_{1\\alpha}} | H_0 \\} = P{\\sqrt{n}D_n \\geq \\lambda_{\\alpha} | H_0} \\approx 1 - K(\\lambda_{\\alpha}) = \\alpha$$  \n",
    "  \n",
    "Тем самым критерий согласия Колмогорова формулируется следюущим образом: если n довольно большое и при выбранном уровне значимости $\\alpha$ число $\\lambda_{\\alpha}$ определено соотношением $K(\\lambda_{\\alpha}) = 1 - \\alpha$, то:  \n",
    "(3)  \n",
    "$$ H_0  \\; отвергается \\Leftrightarrow \\sqrt{n}D_n \\geq \\lambda_{\\alpha}$$  \n",
    "  Наконец, отметим, что для практических вычислений статистики $D_n$ полезна эквивалентная (1) формула $D_n = max(D_n^+, D_n^-)$, где : \n",
    "   \n",
    "$$D_n^+ = \\max_{1 \\leq k \\leq n}(\\frac{k}{n} - F(X_{(k)})), \\; D_n^- = \\max_{1 \\leq k \\leq n}(F(X_{(k)}) - \\frac{k-1}{n})$$  \n",
    " и $X_{(1)} \\leq X_{(2)} \\leq ... \\leq X_{(n)}$  - вариационный ряд выборки $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерий $\\chi^2$ (Пирсона)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, пусть в эксперименте наблюдается дискретная случайная величина $\\xi$, принимающая значения $1,2, ... N$  с некоторыми вероятностями $p_1,...,p_N$, $(p_1 + ... + p_N) = 1$. Если произведено $n$ независимых испытаний над $\\xi$, т.е. имеется выборка $(\\xi_1, ..., \\xi_n)$, то пусть:  \n",
    "$$ \\nu_j = \\sum_{i = 1}^{n} I(\\xi_i = j), j = 1, ..., N$$  \n",
    " -соответствующие частоты исходов $(\\nu_1 + ... + \\nu_N = n)$. Тогда вектор $\\nu = (\\nu_1, ..., \\nu_N)$ имеет полиномиальное распределение $M(n; p_1, ..., p_N)$.    \n",
    " \n",
    "Итак, пусть по наблюдению вектора частот $\\nu = (\\nu_1,..., \\nu_N)$ требуется проверить простую гипотезу $H_0: p = p^0, где p^0 = (p^0_1, ..., p^0_N))$ - заданный вероятностный вектор $(0 < p^0_j < 1, j = 1,..., N, p_1^0 + ...+p_N^0 = 1)$.  \n",
    "К.Пирсон в 1900г. предложил использовать в качестве меры отклонения эмпирических данных (относительных частот $\\nu / n $) от гипотетических значений $p^0$ меру хи-квадрат. Мера $\\chi^2$:  \n",
    "(4)  \n",
    "$$ \\mathring{X_n^2} = \\mathring{X_n^2}(\\nu) = \\sum_{j = 1}^N \\frac{(\\nu_j - np^0_j)^2}{np^0_j} = \\sum_{j = 1}^N \\frac{\\nu_j^2}{np^0_j} - n $$  \n",
    "\n",
    "На этой тестовой статитстике и основывается знаменитый критерий $\\chi^2$ К.Пирсона. В основе этого критерия лежат следующие очевидные соображения. Если гипотеза $H_0$ справедлива, то, поскольку относительная частота $\\nu_j / n$ события $\\{\\xi = j\\}$ является состоятельной оценкой его вероятности $p_j^0 (j = 1,...,n)$, при больших n разности $|\\nu_j / n - p_j^0|$ должны быть малы, следовательно, и значение статистики $\\mathring{X_n^2}$ не должно быть слишком большим. Поэтому естественно задать критическую облать для гипотезы $H_0$ в виде $\\mathcal{T_{1\\alpha}} = \\{\\mathring{X_n^2} > t_{\\alpha}\\}$. Где критическая граница $t_{\\alpha}$ при заданном уровне значимости $\\alpha$ должна быть выбрана из условия:  \n",
    "(5)  \n",
    "$$ P\\{\\mathring{X_n^2} > t_{\\alpha} | H_0 \\} = \\alpha$$\n",
    "  \n",
    "  \n",
    "Теорема 1. Если $0 < p_j^0 < 1, j = 1,..., N$, то при  $n \\rightarrow \\infty$:    \n",
    "$$ \\mathcal{L}(\\mathring{X_n^2}|H_0) \\rightarrow \\chi^2(N - 1)$$  \n",
    "\n",
    "$$ H_0 \\; отвергается \\Leftrightarrow \\{\\mathring{X_n^2} > \\chi^2_{1-\\alpha, N - 1}\\}$$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение задач\n",
    "3.1, 3.3, 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### № 3.1\n",
    "\n",
    "Для данных задачи 1.13 проверить, согласуются ли они с гипотезой $H_0$ о том, что монета была симметричной. Уровень значимости положите равным $\\alpha_1 = 0.05, \\alpha_2 = 0.1$.\n",
    "\n",
    "$Эксперимент: \\; n = 4040, h = 2048, h - выпадение \\; герба$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is 0.7762376237624267 and chi2 for alpha1 = 0.05 is 3.841 -> H0 is true\n",
      "X is 0.7762376237624267 and chi2 for alpha2 = 0.1 is 2.706 -> H0 is true\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# f_obs - Observed frequencies in each category.\n",
    "# f_exp - Expected frequencies in each category.\n",
    "h = 2048\n",
    "n = 4040\n",
    "nu = np.array([h, (n-h)])\n",
    "p = np.array([0.5, 0.5])\n",
    "X = np.sum(nu**2 / (n*p)) - n\n",
    "# degree of freedom is 1 (N = 2)\n",
    "# for alpha_1 = 0.05 chi^2 is 3.841\n",
    "# for alpha_2 = 0.1 chi^2 is 0.0158\n",
    "print(f'X is {X} and chi2 for alpha1 = 0.05 is {3.841} -> H0 is true')\n",
    "print(f'X is {X} and chi2 for alpha2 = 0.1 is {2.706} -> H0 is true')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### № 3.3\n",
    "При $n = 4000$ независимых испытаний события $A_1$, $A_2$, $A_3$, составляющие полную группу, осуществились соответственно $1905, 1015, 1080$ раз. Проверьте, согласуются ли эти данные при уровне значимости $\\alpha = 0.05$ с гипотезой $H_0 = (0.5, 0.25, 0.25)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is 11.137500000000273 and chi2 for alpha = 0.05 is 6.0 -> H0 is false\n"
     ]
    }
   ],
   "source": [
    "n = 4000\n",
    "nu = np.array([1905, 1015, 1080])\n",
    "p = np.array([0.5, 0.25, 0.25])\n",
    "X = np.sum(nu**2 / (n*p)) - n\n",
    "# degree of freedom is 2 (N = 3)\n",
    "# for alpha = 0.05 chi^2 is 6.0\n",
    "print(f'X is {X} and chi2 for alpha = 0.05 is {6.0} -> H0 is false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### №3.5  \n",
    "Согласуются ли данные, приведенные в задачах 1.16 и 1.17, с гипотезой о симметричности костей?  \n",
    "\n",
    "Проводились опыты с бросанием одновременно 12 игральных костей. Наблюдаемую случайную величину $\\xi$ считали равной числу костей, на которых выпало 4,5 или 6 очков. Пусть $h_i$ - число опытов, в которых наблюдалось значение $\\xi = i, i = 0,1,..., 12$. Данные для $n = 4096$ опытов приведены в следующей таблице:\n",
    "\n",
    "\\begin{pmatrix}\n",
    "  i:& 0& 1& 2& 3& 4& 5& 6& 7& 8& 9& 10& 11& 12& \\\\\n",
    "  h_i:& 0& 7& 60& 198& 430& 731& 948& 847& 536& 257& 71& 11& 0&\\\\\n",
    "\\end{pmatrix}\n",
    "\n",
    "\n",
    "Если верна $H_0$ - кости симметричны, то вероятность того, что выпало 4, 5 или 6 очков равна вероятности что выпало 1, 2 или 3 очка и равна 0.5. Тогда распределение $\\xi \\sim Bi(12, 0.5)$. Тогда:  \n",
    "  \n",
    "$p = (0.0002, 0.0029, 0.0161, 0.0537, 0.1208, 0.1934, 0.2256, 0.1934, 0.1208, 0.0537, 0.0161, 0.0029, 0.0002)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is 34.07416568269946 and chi2 for alpha = 0.01 is 26.2 -> H0 is false\n"
     ]
    }
   ],
   "source": [
    "n = 4096\n",
    "nu = np.array([0, 7, 60, 198, 430, 731, 948, 847, 536, 257, 71, 11, 0])\n",
    "p = np.array([0.0002, 0.0029, 0.0161, 0.0537, 0.1208, 0.1934, 0.2258, \n",
    "     0.1934, 0.1208, 0.0537, 0.0161, 0.0029, 0.0002])\n",
    "X = np.sum(nu**2 / (n*p)) - n\n",
    "# degree of freedom is 12 (N = 13)\n",
    "# for alpha = 0.01 is 26.2\n",
    "print(f'X is {X} and chi2 for alpha = 0.01 is {26.2} -> H0 is false')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
